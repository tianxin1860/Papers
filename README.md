 # Papers

 ### Representation Learning

| Paper | 核心思想 | 备注 |
| --- | --- |--- |
|[CoVe](https://einstein.ai/static/images/pages/research/cove/McCann2017LearnedIT.pdf)  | 基于2层双向 LSTM 预训练翻译模型作为 embedding encoder | 2017-NIPS|
|[ELMo](https://arxiv.org/pdf/1802.05365.pdf)  | 基于2层双向 LSTM 预训练 Langeage Model 作为 embedding encoder | 2018-NAACL Best Paper|
|[GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)  | 基于12层 Transformer Decoder 预训练 Langeage Model 作为 embedding encoder | 2018-OpenAI |
|[BERT](https://arxiv.org/pdf/1810.04805.pdf)  |  基于双向 Transformer 预训练 Masked Language Model 作为 embedding encoder |2019-NAACL Best Paper|
|[MT-DNN](https://arxiv.org/pdf/1901.11504.pdf)  |  基于 BERT 利用 multi-task finetune 提升 embedding 的领域泛化性 |2019-arXiv|
